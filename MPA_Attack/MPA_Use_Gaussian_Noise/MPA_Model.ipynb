{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Th∆∞ vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rANUVwZ5z8G1"
   },
   "outputs": [],
   "source": [
    "!pip install -q flwr[simulation]  pandas matplotlib scikit-learn torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hNob57rk0t1z",
    "outputId": "9f326564-c44d-408e-831e-4a6e84cb9190"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Flower 1.13.0 / PyTorch 2.5.1\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import pickle\n",
    "\n",
    "import flwr\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Metrics, Context\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.common import Parameters, Scalar, FitRes, parameters_to_ndarrays\n",
    "from typing import Optional, Union\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Xt47Dal628rM"
   },
   "outputs": [],
   "source": [
    "# C·∫•u h√¨nh thi·∫øt b·ªã (CUDA ho·∫∑c CPU)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "QsCL7xCsGtG1",
    "outputId": "7e41b7c9-e272-4683-9368-0c14f5a7ac97"
   },
   "outputs": [],
   "source": [
    "# ƒê∆∞·ªùng d·∫´n ƒë·∫øn t·∫≠p d·ªØ li·ªáu\n",
    "DATA_PATH = r\"..\\..\\data\\SMSSpamCollection\"\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu\n",
    "data = pd.read_csv(DATA_PATH, sep='\\t', header=None, names=[\"label\", \"text\"])\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi nh√£n 'ham' v√† 'spam' th√†nh 0 v√† 1\n",
    "data[\"label\"] = data[\"label\"].map({\"ham\": 0, \"spam\": 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ITYi82SJJ474"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_CLIENTS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X·ª≠ l√Ω d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QRBk5GL050eJ"
   },
   "outputs": [],
   "source": [
    "class SMSDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vectorizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_vector = self.vectorizer.transform([self.texts[idx]]).toarray().squeeze()\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(text_vector, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def load_datasets(partition_id: int):\n",
    "    print(f\"Loading dataset for partition ID: {partition_id}\")\n",
    "\n",
    "    # Load raw data\n",
    "    data = pd.read_csv(DATA_PATH, sep=\"\\t\", header=None, names=[\"label\", \"text\"])\n",
    "    data[\"label\"] = data[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "    # Vectorize text data\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "    vectorizer.fit(data[\"text\"])\n",
    "\n",
    "    # Split into NUM_CLIENTS partitions\n",
    "    indices = np.arange(len(data))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "    split_indices = np.array_split(indices, NUM_CLIENTS)\n",
    "\n",
    "    # Select partition\n",
    "    partition_indices = split_indices[partition_id]\n",
    "    partition_data = data.iloc[partition_indices]\n",
    "\n",
    "    # Train/val split\n",
    "    train_data, val_data = train_test_split(partition_data, test_size=0.1, random_state=42)\n",
    "    train_dataset = SMSDataset(train_data[\"text\"].tolist(), train_data[\"label\"].tolist(), vectorizer)\n",
    "    val_dataset = SMSDataset(val_data[\"text\"].tolist(), val_data[\"label\"].tolist(), vectorizer)\n",
    "\n",
    "    # Dataloaders\n",
    "    trainloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Test set\n",
    "    test_data = data.iloc[split_indices[-1]]\n",
    "    test_dataset = SMSDataset(test_data[\"text\"].tolist(), test_data[\"label\"].tolist(), vectorizer)\n",
    "    testloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    print(f\"Partition {partition_id}: Train {len(train_data)}, Val {len(val_data)}, Test {len(test_data)}\")\n",
    "    return trainloader, valloader, testloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "PNYRSIi038ff",
    "outputId": "d95aa7b3-8e34-4402-f860-be436ca9cd4e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "wLu3Dfan2FHf"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)  # Fully connected layer\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)  # 2 output classes (ham, spam)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UeW-qbqY2HGu",
    "outputId": "a61bc1ad-a38c-481e-923b-aeb21da99839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset for partition ID: 0\n",
      "Partition 0: Train 502, Val 56, Test 557\n",
      "Net(\n",
      "  (fc1): Linear(in_features=5000, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# L·∫•y trainloader t·ª´ partition ƒë·∫ßu ti√™n\n",
    "trainloader, valloader, testloader = load_datasets(partition_id=0)\n",
    "\n",
    "# S·ªë chi·ªÅu c·ªßa ƒë·∫ßu v√†o t·ª´ vectorizer\n",
    "input_dim = 5000  # (ƒë√£ ƒë∆∞·ª£c ƒë·∫∑t trong load_datasets max_features=5000)\n",
    "\n",
    "# Kh·ªüi t·∫°o m√¥ h√¨nh\n",
    "net = Net(input_dim).to(DEVICE)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "PNKDMzWOCpmP"
   },
   "outputs": [],
   "source": [
    "def train(net, trainloader, epochs: int, verbose=False):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader)\n",
    "        epoch_acc = correct / total\n",
    "        if verbose:\n",
    "            print(f\"Train loss {epoch_loss}, Accuracy {epoch_acc}\")\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(inputs)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi·∫øn thu·∫≠t 1: Nhi·ªÖu Gaussian\n",
    "\n",
    "### M√¥ t·∫£ chi·∫øn thu·∫≠t:\n",
    "Th√™m nhi·ªÖu Gaussian tr·ª±c ti·∫øp v√†o c√°c tham s·ªë ho·∫∑c gradient tr∆∞·ªõc khi g·ª≠i v·ªÅ server.\n",
    "\n",
    "### M·ª•c ti√™u:\n",
    "- L√†m gi·∫£m hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh to√†n c·ª•c.\n",
    "- G√¢y nhi·ªÖu trong qu√° tr√¨nh t·ªïng h·ª£p (aggregation), khi·∫øn m√¥ h√¨nh to√†n c·ª•c kh√¥ng h·ªôi t·ª• ho·∫∑c h·ªôi t·ª• ch·∫≠m.\n",
    "\n",
    "### Chi·∫øn thu·∫≠t c·ª• th·ªÉ:\n",
    "1. **Th√™m nhi·ªÖu nh·ªè (ùúé th·∫•p):**\n",
    "    - M·ª•c ti√™u: L√†m gi·∫£m nh·∫π hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh m√† kh√¥ng g√¢y s·ª± ch√∫ √Ω.\n",
    "\n",
    "2. **Th√™m nhi·ªÖu l·ªõn (ùúé cao):**\n",
    "    - M·ª•c ti√™u: G√¢y r·ªëi lo·∫°n nghi√™m tr·ªçng trong aggregation.\n",
    "\n",
    "3. **Th√™m nhi·ªÖu ch·ªâ v√†o m·ªôt ph·∫ßn c·ªßa tham s·ªë:**\n",
    "    - M·ª•c ti√™u: T·∫•n c√¥ng tinh vi, ch·ªâ g√¢y ·∫£nh h∆∞·ªüng ƒë·∫øn m·ªôt l·ªõp c·ª• th·ªÉ ho·∫∑c m·ªôt nh√≥m tham s·ªë."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "sFjVeY7JKeL6"
   },
   "outputs": [],
   "source": [
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "P1YXP90-C7HZ"
   },
   "outputs": [],
   "source": [
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, net, trainloader, valloader, is_bad_client=False):\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.is_bad_client = is_bad_client  # ƒê√°nh d·∫•u client c√≥ ph·∫£i l√† bad client kh√¥ng\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        set_parameters(self.net, parameters)\n",
    "\n",
    "        # N·∫øu l√† bad client, th·ª±c hi·ªán thao t√°c ƒë·ªôc h·∫°i\n",
    "        if self.is_bad_client:\n",
    "            print(\"Bad client performing model poisoning!\")\n",
    "            # T·∫°o m√¥ h√¨nh ƒë·ªôc h·∫°i ( th√™m nhi·ªÖu Gaussian)\n",
    "            poisoned_parameters = [\n",
    "                # param + np.random.normal(0, 0.1, param.shape)  # Th√™m nhi·ªÖu Gaussian nh·ªè (ùúé th·∫•p)\n",
    "                param + np.random.normal(0, 1, param.shape)  # Nhi·ªÖu l·ªõn\n",
    "                for param in parameters\n",
    "            ]\n",
    "            return poisoned_parameters, len(self.trainloader), {}\n",
    "        \n",
    "\n",
    "        # N·∫øu l√† client th√¥ng th∆∞·ªùng, hu·∫•n luy·ªán b√¨nh th∆∞·ªùng\n",
    "        train(self.net, self.trainloader, epochs=1)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "al49ggCSC7oK"
   },
   "outputs": [],
   "source": [
    "def client_fn(context: Context) -> Client:\n",
    "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
    "    input_dim = 5000  # ƒê√£ ƒë∆∞·ª£c ƒë·∫∑t trong load_datasets max_features=5000\n",
    "    net = Net(input_dim).to(DEVICE)\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    trainloader, valloader, _ = load_datasets(partition_id=partition_id)\n",
    "\n",
    "    # ƒê·ªãnh nghƒ©a c√°c client ƒë·ªôc h·∫°i (v√≠ d·ª•: ch·ªâ ƒë·ªãnh partition ID l√† bad client)\n",
    "    bad_clients = [1, 3, 5]  # Danh s√°ch c√°c partition ID c·ªßa client ƒë·ªôc h·∫°i\n",
    "    is_bad_client = partition_id in bad_clients\n",
    "\n",
    "    trainloader, valloader, _ = load_datasets(partition_id=partition_id)\n",
    "\n",
    "    return FlowerClient(net, trainloader, valloader, is_bad_client).to_client()\n",
    "\n",
    "# Create the ClientApp\n",
    "client = ClientApp(client_fn=client_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(metrics: List[Tuple[int, dict]]) -> dict:\n",
    "    # Ensure there are metrics to aggregate\n",
    "    if not metrics:\n",
    "        return {}\n",
    "\n",
    "    # Initialize storage for weighted sums\n",
    "    weighted_sums = {}\n",
    "    total_examples = 0\n",
    "\n",
    "    for num_examples, metric_dict in metrics:\n",
    "        total_examples += num_examples\n",
    "        for key, value in metric_dict.items():\n",
    "            if key not in weighted_sums:\n",
    "                weighted_sums[key] = 0\n",
    "            weighted_sums[key] += num_examples * value\n",
    "\n",
    "    # Compute weighted averages\n",
    "    aggregated_metrics = {\n",
    "        key: weighted_sums[key] / total_examples for key in weighted_sums\n",
    "    }\n",
    "    return aggregated_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom SaveModelStrategy implementation\n",
    "class SaveModelStrategy(FedAvg):\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: list[tuple[ClientProxy, FitRes]],\n",
    "        failures: list[Union[tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> tuple[Optional[Parameters], dict[str, Scalar]]:\n",
    "\n",
    "        # Call aggregate_fit from the base class (FedAvg)\n",
    "        aggregated_parameters, aggregated_metrics = super().aggregate_fit(\n",
    "            server_round, results, failures\n",
    "        )\n",
    "\n",
    "        if aggregated_parameters is not None:\n",
    "            # Convert `Parameters` to `list[np.ndarray]`\n",
    "            aggregated_ndarrays = parameters_to_ndarrays(aggregated_parameters)\n",
    "\n",
    "            # Save aggregated weights for each round\n",
    "            print(f\"Saving round {server_round} aggregated weights...\")\n",
    "            np.savez(f\"round-{server_round}-weights.npz\", *aggregated_ndarrays)\n",
    "\n",
    "            # Save the MPA model at the end of training\n",
    "            if server_round == 5:\n",
    "                with open(\"MPA_model.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(aggregated_ndarrays, f)\n",
    "                print(\"MPA model saved as 'MPA_model.pkl'\")\n",
    "                # L∆∞u d∆∞·ªõi d·∫°ng PyTorch\n",
    "                torch.save(aggregated_ndarrays, \"MPA_model.pth\")\n",
    "                print(\"MPA model saved as 'MPA_model.pth'\")\n",
    "\n",
    "        return aggregated_parameters, aggregated_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "eadAnMzoDEk8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the server function\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    # Use the custom SaveModelStrategy\n",
    "    strategy = SaveModelStrategy(\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=0.5,\n",
    "        min_fit_clients=10,\n",
    "        min_evaluate_clients=5,\n",
    "        min_available_clients=10,\n",
    "        fit_metrics_aggregation_fn=weighted_average,\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,\n",
    "    )\n",
    "\n",
    "    # Configure the server for 5 rounds of training\n",
    "    config = ServerConfig(num_rounds=5)\n",
    "\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "\n",
    "# Create a new server instance with the SaveModelStrategy\n",
    "server = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "3tzMj2s_DJRz"
   },
   "outputs": [],
   "source": [
    "# Specify the resources each of your clients need\n",
    "# By default, each client will be allocated 1x CPU and 0x GPUs\n",
    "backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 0.0}}\n",
    "\n",
    "# When running on GPU, assign an entire GPU for each client\n",
    "if DEVICE.type == \"cuda\":\n",
    "    backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 1.0}}\n",
    "    # Refer to our Flower framework documentation for more details about Flower simulations\n",
    "    # and how to set up the `backend_config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Disable oneDNN custom operations to avoid floating-point round-off errors\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "# Import TensorFlow and other required libraries\n",
    "import tensorflow as tf\n",
    "\n",
    "# Initialize TensorFlow\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "offZbgFTDHis",
    "outputId": "a93523eb-ae4e-4594-f64e-6cc100855411"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94mDEBUG 2024-12-01 01:13:39,053\u001b[0m:     Asyncio event loop already running.\n",
      "\u001b[94mDEBUG 2024-12-01 01:13:39,054\u001b[0m:     Logger propagate set to False\n",
      "\u001b[94mDEBUG 2024-12-01 01:13:39,055\u001b[0m:     Pre-registering run with id 8019549350306750640\n",
      "\u001b[94mDEBUG 2024-12-01 01:13:39,055\u001b[0m:     Using InMemoryState\n",
      "\u001b[94mDEBUG 2024-12-01 01:13:39,056\u001b[0m:     Using InMemoryState\n",
      "\u001b[92mINFO 2024-12-01 01:13:39,073\u001b[0m:      Starting Flower ServerApp, config: num_rounds=5, no round_timeout\n",
      "\u001b[94mDEBUG 2024-12-01 01:13:39,091\u001b[0m:     Buffer time delay: 5s\n",
      "\u001b[92mINFO 2024-12-01 01:13:39,324\u001b[0m:      \n",
      "\u001b[92mINFO 2024-12-01 01:13:39,328\u001b[0m:      [INIT]\n",
      "\u001b[92mINFO 2024-12-01 01:13:39,329\u001b[0m:      Requesting initial parameters from one random client\n",
      "\u001b[94mDEBUG 2024-12-01 01:13:44,331\u001b[0m:     Using InMemoryState\n",
      "\u001b[94mDEBUG 2024-12-01 01:13:44,332\u001b[0m:     Registered 10 nodes\n",
      "\u001b[94mDEBUG 2024-12-01 01:13:44,333\u001b[0m:     Supported backends: ['ray']\n",
      "\u001b[94mDEBUG 2024-12-01 01:13:44,333\u001b[0m:     Initialising: RayBackend\n",
      "\u001b[94mDEBUG 2024-12-01 01:13:44,334\u001b[0m:     Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {}, 'actor': {'tensorflow': 0}}\n",
      "2024-12-01 01:13:48,542\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "\u001b[94mDEBUG 2024-12-01 01:13:51,810\u001b[0m:     Constructed ActorPool with: 12 actors\n",
      "\u001b[94mDEBUG 2024-12-01 01:13:51,811\u001b[0m:     Using InMemoryState\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=30364)\u001b[0m Loading dataset for partition ID: 7\n",
      "\u001b[36m(ClientAppActor pid=30364)\u001b[0m Partition 7: Train 501, Val 56, Test 557\n",
      "\u001b[36m(ClientAppActor pid=30364)\u001b[0m Loading dataset for partition ID: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO 2024-12-01 01:14:11,474\u001b[0m:      Received initial parameters from one random client\n",
      "\u001b[92mINFO 2024-12-01 01:14:11,474\u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO 2024-12-01 01:14:11,475\u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO 2024-12-01 01:14:11,475\u001b[0m:      \n",
      "\u001b[92mINFO 2024-12-01 01:14:11,476\u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO 2024-12-01 01:14:11,476\u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=30364)\u001b[0m Partition 7: Train 501, Val 56, Test 557\n",
      "\u001b[36m(ClientAppActor pid=30364)\u001b[0m Loading dataset for partition ID: 0\n",
      "\u001b[36m(ClientAppActor pid=30364)\u001b[0m Partition 0: Train 502, Val 56, Test 557\n",
      "\u001b[36m(ClientAppActor pid=30364)\u001b[0m Loading dataset for partition ID: 0\n",
      "\u001b[36m(ClientAppActor pid=30364)\u001b[0m Partition 0: Train 502, Val 56, Test 557\n",
      "\u001b[36m(ClientAppActor pid=24808)\u001b[0m Loading dataset for partition ID: 5\n",
      "\u001b[36m(ClientAppActor pid=1956)\u001b[0m Loading dataset for partition ID: 4\n",
      "\u001b[36m(ClientAppActor pid=1956)\u001b[0m Partition 4: Train 501, Val 56, Test 557\n",
      "\u001b[36m(ClientAppActor pid=24808)\u001b[0m Bad client performing model poisoning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO 2024-12-01 01:14:24,104\u001b[0m:      aggregate_fit: received 10 results and 0 failures\n",
      "\u001b[92mINFO 2024-12-01 01:14:24,165\u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 1 aggregated weights...\n",
      "\u001b[36m(ClientAppActor pid=8544)\u001b[0m Loading dataset for partition ID: 1\u001b[32m [repeated 17x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO 2024-12-01 01:14:24,661\u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO 2024-12-01 01:14:24,661\u001b[0m:      \n",
      "\u001b[92mINFO 2024-12-01 01:14:24,662\u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO 2024-12-01 01:14:24,662\u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=8544)\u001b[0m Partition 1: Train 502, Val 56, Test 557\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21160)\u001b[0m Bad client performing model poisoning!\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO 2024-12-01 01:14:28,985\u001b[0m:      aggregate_fit: received 10 results and 0 failures\n",
      "\u001b[92mINFO 2024-12-01 01:14:29,042\u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 2 aggregated weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO 2024-12-01 01:14:29,575\u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO 2024-12-01 01:14:29,575\u001b[0m:      \n",
      "\u001b[92mINFO 2024-12-01 01:14:29,577\u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO 2024-12-01 01:14:29,578\u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=30492)\u001b[0m Loading dataset for partition ID: 9\u001b[32m [repeated 35x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=30364)\u001b[0m Partition 8: Train 501, Val 56, Test 557\u001b[32m [repeated 34x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=23904)\u001b[0m Bad client performing model poisoning!\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO 2024-12-01 01:14:32,149\u001b[0m:      aggregate_fit: received 10 results and 0 failures\n",
      "\u001b[92mINFO 2024-12-01 01:14:32,204\u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 3 aggregated weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO 2024-12-01 01:14:32,901\u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO 2024-12-01 01:14:32,903\u001b[0m:      \n",
      "\u001b[92mINFO 2024-12-01 01:14:32,903\u001b[0m:      [ROUND 4]\n",
      "\u001b[92mINFO 2024-12-01 01:14:32,904\u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n",
      "\u001b[92mINFO 2024-12-01 01:14:35,263\u001b[0m:      aggregate_fit: received 10 results and 0 failures\n",
      "\u001b[92mINFO 2024-12-01 01:14:35,304\u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 4 aggregated weights...\n",
      "\u001b[36m(ClientAppActor pid=20704)\u001b[0m Loading dataset for partition ID: 2\u001b[32m [repeated 55x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=24808)\u001b[0m Partition 8: Train 501, Val 56, Test 557\u001b[32m [repeated 50x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=30492)\u001b[0m Bad client performing model poisoning!\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO 2024-12-01 01:14:35,973\u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO 2024-12-01 01:14:35,974\u001b[0m:      \n",
      "\u001b[92mINFO 2024-12-01 01:14:35,974\u001b[0m:      [ROUND 5]\n",
      "\u001b[92mINFO 2024-12-01 01:14:35,975\u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n",
      "\u001b[92mINFO 2024-12-01 01:14:38,536\u001b[0m:      aggregate_fit: received 10 results and 0 failures\n",
      "\u001b[92mINFO 2024-12-01 01:14:38,640\u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 5 aggregated weights...\n",
      "MPA model saved as 'MPA_model.pkl'\n",
      "MPA model saved as 'MPA_model.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO 2024-12-01 01:14:39,500\u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO 2024-12-01 01:14:39,504\u001b[0m:      \n",
      "\u001b[92mINFO 2024-12-01 01:14:39,506\u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO 2024-12-01 01:14:39,506\u001b[0m:      Run finished 5 round(s) in 28.03s\n",
      "\u001b[92mINFO 2024-12-01 01:14:39,519\u001b[0m:      \tHistory (loss, distributed):\n",
      "\u001b[92mINFO 2024-12-01 01:14:39,521\u001b[0m:      \t\tround 1: 0.7337283283472061\n",
      "\u001b[92mINFO 2024-12-01 01:14:39,522\u001b[0m:      \t\tround 2: 3.314071536064148\n",
      "\u001b[92mINFO 2024-12-01 01:14:39,523\u001b[0m:      \t\tround 3: 8.764304435253143\n",
      "\u001b[92mINFO 2024-12-01 01:14:39,525\u001b[0m:      \t\tround 4: 15.419864845275878\n",
      "\u001b[92mINFO 2024-12-01 01:14:39,526\u001b[0m:      \t\tround 5: 33.48049692530185\n",
      "\u001b[92mINFO 2024-12-01 01:14:39,527\u001b[0m:      \tHistory (metrics, distributed, evaluate):\n",
      "\u001b[92mINFO 2024-12-01 01:14:39,528\u001b[0m:      \t{'accuracy': [(1, 0.6142857142857142),\n",
      "\u001b[92mINFO 2024-12-01 01:14:39,528\u001b[0m:      \t              (2, 0.5785714285714285),\n",
      "\u001b[92mINFO 2024-12-01 01:14:39,529\u001b[0m:      \t              (3, 0.6571428571428573),\n",
      "\u001b[92mINFO 2024-12-01 01:14:39,530\u001b[0m:      \t              (4, 0.7285714285714286),\n",
      "\u001b[92mINFO 2024-12-01 01:14:39,531\u001b[0m:      \t              (5, 0.7392857142857143)]}\n",
      "\u001b[92mINFO 2024-12-01 01:14:39,531\u001b[0m:      \n",
      "\u001b[94mDEBUG 2024-12-01 01:14:39,550\u001b[0m:     ServerApp finished running.\n",
      "\u001b[94mDEBUG 2024-12-01 01:14:39,559\u001b[0m:     ServerApp finished running.\n",
      "\u001b[94mDEBUG 2024-12-01 01:14:39,561\u001b[0m:     Triggered stop event for Simulation Engine.\n",
      "\u001b[94mDEBUG 2024-12-01 01:14:40,490\u001b[0m:     Terminated 12 actors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=21160)\u001b[0m Loading dataset for partition ID: 8\u001b[32m [repeated 39x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21160)\u001b[0m Partition 8: Train 501, Val 56, Test 557\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=30492)\u001b[0m Bad client performing model poisoning!\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94mDEBUG 2024-12-01 01:14:41,525\u001b[0m:     Terminated RayBackend\n",
      "\u001b[94mDEBUG 2024-12-01 01:14:41,526\u001b[0m:     Stopping Simulation Engine now.\n"
     ]
    }
   ],
   "source": [
    "# Run simulation\n",
    "history = run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config,\n",
    "    verbose_logging=True\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "spam_email_detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
